{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n337KoD2om3L",
        "outputId": "97ada0c6-f21c-483e-d63d-08abddd49004"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Feb 18 21:56:31 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\n",
            "| N/A   44C    P8    15W /  60W |      8MiB /  6144MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1614      G   /usr/lib/xorg/Xorg                  4MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "\n",
        "# If this doesn't work, there's no GPU available or detected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLJAcUHpvmp4",
        "outputId": "95bcda95-a484-40c6-e5a7-47f4378759a8"
      },
      "outputs": [],
      "source": [
        "!pip install audiolm-pytorch"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xuNcsDJsvQwh"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBxNK5cKW--_"
      },
      "source": [
        "### Imports & paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrNeKngVVM0L"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import math\n",
        "import wave\n",
        "import struct\n",
        "import os\n",
        "import urllib.request\n",
        "import tarfile\n",
        "from audiolm_pytorch import SoundStream, SoundStreamTrainer, HubertWithKmeans, SemanticTransformer, SemanticTransformerTrainer, HubertWithKmeans, CoarseTransformer, CoarseTransformerWrapper, CoarseTransformerTrainer, FineTransformer, FineTransformerWrapper, FineTransformerTrainer, AudioLM\n",
        "from torch import nn\n",
        "import torch\n",
        "import torchaudio\n",
        "\n",
        "\n",
        "# define all dataset paths, checkpoints, etc\n",
        "dataset_folder = \"../../datasets/fma_medium/\"\n",
        "soundstream_ckpt = \"runs/soundstream.8.pt\" # this can change depending on number of steps\n",
        "hubert_ckpt = 'hubert/hubert_base_ls960.pt'\n",
        "hubert_quantizer = f'hubert/hubert_base_ls960_L9_km500.bin' # listed in row \"HuBERT Base (~95M params)\", column Quantizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYcI0aXEwuxR"
      },
      "source": [
        "## Training\n",
        "\n",
        "Now that we have a dataset, we can train AudioLM.\n",
        "\n",
        "**Note**: do NOT type \"y\" to overwrite previous experiments/ checkpoints when running through the cells here unless you're ready to the entire results folder! Otherwise you will end up erasing things (e.g. you train SoundStream first, and if you choose \"overwrite\" then you lose the SoundStream checkpoint when you then train SemanticTransformer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7GiyBcBWiZV"
      },
      "source": [
        "### SoundStream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGU0OZiOwPEO",
        "outputId": "21dd959c-6458-4477-8403-cf810166f38d"
      },
      "outputs": [],
      "source": [
        "soundstream = SoundStream(\n",
        "    codebook_size = 1024,\n",
        "    rq_num_quantizers = 8,\n",
        ")\n",
        "\n",
        "soundstream_path = \"runs/soundstream.2000.pt\"\n",
        "soundstream.load_from_trainer_saved_obj(f\"./{soundstream_path}\")  #Load pretrained\n",
        "\n",
        "trainer = SoundStreamTrainer(\n",
        "    soundstream,\n",
        "    folder = dataset_folder,\n",
        "    batch_size = 4,\n",
        "    grad_accum_every = 8,         # effective batch size of 32\n",
        "    data_max_length = 320 * 32,\n",
        "    save_results_every = 2,\n",
        "    save_model_every = 4,\n",
        "    num_train_steps = 1\n",
        ").cuda()\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SoundStream Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from audiolm_pytorch.data import SoundDataset, get_dataloader\n",
        "from audiolm_pytorch.trainer import cycle\n",
        "\n",
        "soundstream = SoundStream(\n",
        "    codebook_size = 1024,\n",
        "    rq_num_quantizers = 8,\n",
        ")\n",
        "soundstream_path = \"runs/soundstream.2000.pt\"\n",
        "soundstream.load_from_trainer_saved_obj(f\"./{soundstream_path}\")  #Load ckpt for test\n",
        "\n",
        "soundstream.eval()\n",
        "soundstream = soundstream.to(\"cuda\")\n",
        "\n",
        "ds = SoundDataset(\n",
        "            folder = dataset_folder,\n",
        "            max_length = 24000*15,  #15 Sec\n",
        "            target_sample_hz = soundstream.target_sample_hz,\n",
        "            seq_len_multiple_of = soundstream.seq_len_multiple_of\n",
        ")\n",
        "\n",
        "dl = get_dataloader(ds, batch_size = 1, num_workers = 0, shuffle = True)\n",
        "\n",
        "dl_iter = cycle(dl)\n",
        "\n",
        "wave, = next(dl_iter)\n",
        "wave = wave.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "results_folder = Path('./results_test')\n",
        "results_folder.mkdir(exist_ok=True)\n",
        "\n",
        "filename = str(results_folder / f'orig_{0}.wav')\n",
        "torchaudio.save(filename, wave.cpu(), soundstream.target_sample_hz)\n",
        "\n",
        "with torch.no_grad():\n",
        "    recons = soundstream(wave, return_recons_only = True)\n",
        "\n",
        "for ind, recon in enumerate(recons.unbind(dim = 0)):\n",
        "    filename = str(results_folder / f'sample_{ind}.wav')\n",
        "    torchaudio.save(filename, recon.cpu().detach(), soundstream.target_sample_hz)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqjN28L4Wc5Q"
      },
      "source": [
        "### SemanticTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgd962eSvDzS",
        "outputId": "b0550cde-0c8b-4a39-f896-f6f813f50f8c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MiniBatchKMeans from version 0.24.0 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training with dataset of 2 samples and validating with randomly splitted 1 samples\n",
            "do you want to clear previous experiment checkpoints and results? (y/n) n\n",
            "0: loss: 6.648584365844727\n",
            "0: valid loss 5.763116359710693\n",
            "0: saving model to results\n",
            "training complete\n"
          ]
        }
      ],
      "source": [
        "# hubert checkpoints can be downloaded at\n",
        "# https://github.com/facebookresearch/fairseq/tree/main/examples/hubert\n",
        "if not os.path.isdir(\"hubert\"):\n",
        "  os.makedirs(\"hubert\")\n",
        "if not os.path.isfile(hubert_ckpt):\n",
        "  hubert_ckpt_download = f\"https://dl.fbaipublicfiles.com/{hubert_ckpt}\"\n",
        "  urllib.request.urlretrieve(hubert_ckpt_download, f\"./{hubert_ckpt}\")\n",
        "if not os.path.isfile(hubert_quantizer):\n",
        "  hubert_quantizer_download = f\"https://dl.fbaipublicfiles.com/{hubert_quantizer}\"\n",
        "  urllib.request.urlretrieve(hubert_quantizer_download, f\"./{hubert_quantizer}\")\n",
        "\n",
        "wav2vec = HubertWithKmeans(\n",
        "    checkpoint_path = f'./{hubert_ckpt}',\n",
        "    kmeans_path = f'./{hubert_quantizer}'\n",
        ")\n",
        "\n",
        "semantic_transformer = SemanticTransformer(\n",
        "    num_semantic_tokens = wav2vec.codebook_size,\n",
        "    dim = 1024,\n",
        "    depth = 6\n",
        ").cuda()\n",
        "\n",
        "\n",
        "trainer = SemanticTransformerTrainer(\n",
        "    transformer = semantic_transformer,\n",
        "    wav2vec = wav2vec,\n",
        "    folder = dataset_folder,\n",
        "    batch_size = 1,\n",
        "    data_max_length = 320 * 32,\n",
        "    num_train_steps = 1\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eEvIzhEWwRz"
      },
      "source": [
        "### CoarseTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LeWmaNHzzY9",
        "outputId": "7e7ecb3b-f59e-4d18-c8c9-64762e9b43fc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MiniBatchKMeans from version 0.24.0 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training with dataset of 2 samples and validating with randomly splitted 1 samples\n",
            "do you want to clear previous experiment checkpoints and results? (y/n) n\n",
            "0: loss: 63.983970642089844\n",
            "0: valid loss 63.398582458496094\n",
            "0: saving model to results\n",
            "1: loss: 65.85967254638672\n",
            "2: loss: 62.4722900390625\n",
            "2: valid loss 50.01605987548828\n",
            "3: loss: 11.735434532165527\n",
            "4: loss: 3.976104497909546\n",
            "4: valid loss 46.094608306884766\n",
            "4: saving model to results\n",
            "5: loss: 58.27140426635742\n",
            "6: loss: 41.68347930908203\n",
            "6: valid loss 45.54595184326172\n",
            "7: loss: 2.2387890815734863\n",
            "8: loss: 0.4718627631664276\n",
            "8: valid loss 39.10848617553711\n",
            "8: saving model to results\n",
            "training complete\n"
          ]
        }
      ],
      "source": [
        "wav2vec = HubertWithKmeans(\n",
        "    checkpoint_path = f'./{hubert_ckpt}',\n",
        "    kmeans_path = f'./{hubert_quantizer}'\n",
        ")\n",
        "\n",
        "soundstream = SoundStream(\n",
        "    codebook_size = 1024,\n",
        "    rq_num_quantizers = 8,\n",
        ")\n",
        "\n",
        "soundstream.load(f\"./{soundstream_ckpt}\")\n",
        "\n",
        "coarse_transformer = CoarseTransformer(\n",
        "    num_semantic_tokens = wav2vec.codebook_size,\n",
        "    codebook_size = 1024,\n",
        "    num_coarse_quantizers = 3,\n",
        "    dim = 512,\n",
        "    depth = 6\n",
        ")\n",
        "\n",
        "trainer = CoarseTransformerTrainer(\n",
        "    transformer = coarse_transformer,\n",
        "    soundstream = soundstream,\n",
        "    wav2vec = wav2vec,\n",
        "    folder = dataset_folder,\n",
        "    batch_size = 1,\n",
        "    data_max_length = 320 * 32,\n",
        "    save_results_every = 2,\n",
        "    save_model_every = 4,\n",
        "    num_train_steps = 9\n",
        ")\n",
        "# NOTE: I changed num_train_steps to 9 (aka 8 + 1) from 10000 to make things go faster for demo purposes\n",
        "# adjusting save_*_every variables for the same reason\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRvj7qOJWzmw"
      },
      "source": [
        "### FineTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRaEhRRKWg8F",
        "outputId": "7cc166c4-c8e9-45ef-8293-8f5381c2d3af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training with dataset of 2 samples and validating with randomly splitted 1 samples\n",
            "do you want to clear previous experiment checkpoints and results? (y/n) n\n",
            "0: loss: 70.90608215332031\n",
            "0: valid loss 65.99951171875\n",
            "0: saving model to results\n",
            "1: loss: 43.6014289855957\n",
            "2: loss: 8.300681114196777\n",
            "3: loss: 61.23375701904297\n",
            "4: loss: 63.34052276611328\n",
            "5: loss: 2.010118246078491\n",
            "6: loss: 56.52588653564453\n",
            "7: loss: 0.5423888564109802\n",
            "8: loss: 0.005095238331705332\n",
            "training complete\n"
          ]
        }
      ],
      "source": [
        "soundstream = SoundStream(\n",
        "    codebook_size = 1024,\n",
        "    rq_num_quantizers = 8,\n",
        ")\n",
        "\n",
        "soundstream.load(f\"./{soundstream_ckpt}\")\n",
        "\n",
        "fine_transformer = FineTransformer(\n",
        "    num_coarse_quantizers = 3,\n",
        "    num_fine_quantizers = 5,\n",
        "    codebook_size = 1024,\n",
        "    dim = 512,\n",
        "    depth = 6\n",
        ")\n",
        "\n",
        "trainer = FineTransformerTrainer(\n",
        "    transformer = fine_transformer,\n",
        "    soundstream = soundstream,\n",
        "    folder = dataset_folder,\n",
        "    batch_size = 1,\n",
        "    data_max_length = 320 * 32,\n",
        "    num_train_steps = 9\n",
        ")\n",
        "# NOTE: I changed num_train_steps to 9 (aka 8 + 1) from 10000 to make things go faster for demo purposes\n",
        "# adjusting save_*_every variables for the same reason\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoHgkgA3XKXH"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzghrux5WinW",
        "outputId": "9dd39f7f-0046-4a5f-826e-a442345987af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "generating semantic:   0%|          | 10/2048 [00:00<00:25, 78.55it/s]\n",
            "generating coarse: 100%|██████████| 512/512 [00:14<00:00, 34.83it/s]\n",
            "generating fine: 100%|██████████| 512/512 [02:56<00:00,  2.91it/s]\n"
          ]
        }
      ],
      "source": [
        "# Everything together\n",
        "audiolm = AudioLM(\n",
        "    wav2vec = wav2vec,\n",
        "    soundstream = soundstream,\n",
        "    semantic_transformer = semantic_transformer,\n",
        "    coarse_transformer = coarse_transformer,\n",
        "    fine_transformer = fine_transformer\n",
        ")\n",
        "\n",
        "generated_wav = audiolm(batch_size = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "4rQPHTSRngEr"
      },
      "outputs": [],
      "source": [
        "output_path = \"out.wav\"\n",
        "sample_rate = 44100\n",
        "torchaudio.save(output_path, generated_wav.cpu(), sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "is9wLY_ncDYK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "MusicEnv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "bffe47f1c9f0e7fbf3f698259a3ec592739737d187a49ddf7120ee60a480d3a6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
