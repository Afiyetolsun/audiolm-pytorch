{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n337KoD2om3L",
        "outputId": "97ada0c6-f21c-483e-d63d-08abddd49004"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Feb 18 21:56:31 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\n",
            "| N/A   44C    P8    15W /  60W |      8MiB /  6144MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1614      G   /usr/lib/xorg/Xorg                  4MiB |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "\n",
        "# If this doesn't work, there's no GPU available or detected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLJAcUHpvmp4",
        "outputId": "95bcda95-a484-40c6-e5a7-47f4378759a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting audiolm-pytorch\n",
            "  Using cached audiolm_pytorch-0.14.3-py3-none-any.whl (29 kB)\n",
            "Collecting accelerate\n",
            "  Using cached accelerate-0.16.0-py3-none-any.whl (199 kB)\n",
            "Collecting joblib\n",
            "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "Collecting fairseq\n",
            "  Using cached fairseq-0.12.2-cp310-cp310-linux_x86_64.whl\n",
            "Collecting Mega-pytorch\n",
            "  Using cached Mega_pytorch-0.0.15-py3-none-any.whl (6.4 kB)\n",
            "Collecting beartype\n",
            "  Using cached beartype-0.12.0-py3-none-any.whl (754 kB)\n",
            "Collecting ema-pytorch\n",
            "  Using cached ema_pytorch-0.2.1-py3-none-any.whl (4.4 kB)\n",
            "Collecting lion-pytorch\n",
            "  Using cached lion_pytorch-0.0.6-py3-none-any.whl (4.2 kB)\n",
            "Collecting torch>=1.12\n",
            "  Using cached torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "Collecting torchaudio\n",
            "  Using cached torchaudio-0.13.1-cp310-cp310-manylinux1_x86_64.whl (4.2 MB)\n",
            "Collecting tqdm\n",
            "  Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
            "Collecting sentencepiece\n",
            "  Using cached sentencepiece-0.1.97-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "Collecting scikit-learn\n",
            "  Using cached scikit_learn-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
            "Collecting local-attention>=1.6.0\n",
            "  Using cached local_attention-1.6.0-py3-none-any.whl (7.0 kB)\n",
            "Collecting vector-quantize-pytorch>=0.10.15\n",
            "  Using cached vector_quantize_pytorch-1.0.1-py3-none-any.whl (8.7 kB)\n",
            "Collecting einops>=0.6\n",
            "  Using cached einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "Collecting typing-extensions\n",
            "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "Collecting wheel\n",
            "  Using cached wheel-0.38.4-py3-none-any.whl (36 kB)\n",
            "Collecting setuptools\n",
            "  Using cached setuptools-67.3.2-py3-none-any.whl (1.1 MB)\n",
            "Collecting pyyaml\n",
            "  Using cached PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
            "Collecting psutil\n",
            "  Using cached psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
            "Collecting packaging>=20.0\n",
            "  Using cached packaging-23.0-py3-none-any.whl (42 kB)\n",
            "Collecting numpy>=1.17\n",
            "  Using cached numpy-1.24.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "Collecting cffi\n",
            "  Using cached cffi-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB)\n",
            "Collecting bitarray\n",
            "  Using cached bitarray-2.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (270 kB)\n",
            "Collecting omegaconf<2.1\n",
            "  Using cached omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Collecting cython\n",
            "  Using cached Cython-0.29.33-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
            "Collecting regex\n",
            "  Using cached regex-2022.10.31-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\n",
            "Collecting sacrebleu>=1.4.12\n",
            "  Using cached sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "Collecting hydra-core<1.1,>=1.0.7\n",
            "  Using cached hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "Collecting scipy\n",
            "  Using cached scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Using cached tokenizers-0.13.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "Collecting requests\n",
            "  Using cached requests-2.28.2-py3-none-any.whl (62 kB)\n",
            "Collecting filelock\n",
            "  Using cached filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Using cached huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Using cached antlr4_python3_runtime-4.8-py3-none-any.whl\n",
            "Collecting lxml\n",
            "  Using cached lxml-4.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (7.1 MB)\n",
            "Collecting colorama\n",
            "  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting portalocker\n",
            "  Using cached portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tabulate>=0.8.9\n",
            "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Collecting pycparser\n",
            "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
            "Collecting idna<4,>=2.5\n",
            "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
            "Collecting charset-normalizer<4,>=2\n",
            "  Using cached charset_normalizer-3.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (198 kB)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Using cached urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Using cached certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
            "Installing collected packages: tokenizers, sentencepiece, charset-normalizer, bitarray, antlr4-python3-runtime, wheel, urllib3, typing-extensions, tqdm, threadpoolctl, tabulate, setuptools, regex, pyyaml, pycparser, psutil, portalocker, packaging, nvidia-cuda-nvrtc-cu11, numpy, lxml, joblib, idna, filelock, einops, cython, colorama, certifi, beartype, scipy, sacrebleu, requests, omegaconf, nvidia-cuda-runtime-cu11, nvidia-cublas-cu11, cffi, scikit-learn, nvidia-cudnn-cu11, hydra-core, huggingface-hub, transformers, torch, vector-quantize-pytorch, torchaudio, Mega-pytorch, local-attention, lion-pytorch, ema-pytorch, accelerate, fairseq, audiolm-pytorch\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.13.2\n",
            "    Uninstalling tokenizers-0.13.2:\n",
            "      Successfully uninstalled tokenizers-0.13.2\n",
            "  Attempting uninstall: sentencepiece\n",
            "    Found existing installation: sentencepiece 0.1.97\n",
            "    Uninstalling sentencepiece-0.1.97:\n",
            "      Successfully uninstalled sentencepiece-0.1.97\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.0.1\n",
            "    Uninstalling charset-normalizer-3.0.1:\n",
            "      Successfully uninstalled charset-normalizer-3.0.1\n",
            "  Attempting uninstall: bitarray\n",
            "    Found existing installation: bitarray 2.7.2\n",
            "    Uninstalling bitarray-2.7.2:\n",
            "      Successfully uninstalled bitarray-2.7.2\n",
            "  Attempting uninstall: antlr4-python3-runtime\n",
            "    Found existing installation: antlr4-python3-runtime 4.8\n",
            "    Uninstalling antlr4-python3-runtime-4.8:\n",
            "      Successfully uninstalled antlr4-python3-runtime-4.8\n",
            "  Attempting uninstall: wheel\n",
            "    Found existing installation: wheel 0.38.4\n",
            "    Uninstalling wheel-0.38.4:\n",
            "      Successfully uninstalled wheel-0.38.4\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.14\n",
            "    Uninstalling urllib3-1.26.14:\n",
            "      Successfully uninstalled urllib3-1.26.14\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.1\n",
            "    Uninstalling tqdm-4.64.1:\n",
            "      Successfully uninstalled tqdm-4.64.1\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.1.0\n",
            "    Uninstalling threadpoolctl-3.1.0:\n",
            "      Successfully uninstalled threadpoolctl-3.1.0\n",
            "  Attempting uninstall: tabulate\n",
            "    Found existing installation: tabulate 0.9.0\n",
            "    Uninstalling tabulate-0.9.0:\n",
            "      Successfully uninstalled tabulate-0.9.0\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.3.2\n",
            "    Uninstalling setuptools-67.3.2:\n",
            "      Successfully uninstalled setuptools-67.3.2\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2022.10.31\n",
            "    Uninstalling regex-2022.10.31:\n",
            "      Successfully uninstalled regex-2022.10.31\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "  Attempting uninstall: pycparser\n",
            "    Found existing installation: pycparser 2.21\n",
            "    Uninstalling pycparser-2.21:\n",
            "      Successfully uninstalled pycparser-2.21\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.4\n",
            "    Uninstalling psutil-5.9.4:\n",
            "      Successfully uninstalled psutil-5.9.4\n",
            "  Attempting uninstall: portalocker\n",
            "    Found existing installation: portalocker 2.7.0\n",
            "    Uninstalling portalocker-2.7.0:\n",
            "      Successfully uninstalled portalocker-2.7.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 23.0\n",
            "    Uninstalling packaging-23.0:\n",
            "      Successfully uninstalled packaging-23.0\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu11\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu11 11.7.99\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu11-11.7.99:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu11-11.7.99\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.9.2\n",
            "    Uninstalling lxml-4.9.2:\n",
            "      Successfully uninstalled lxml-4.9.2\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.2.0\n",
            "    Uninstalling joblib-1.2.0:\n",
            "      Successfully uninstalled joblib-1.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.9.0\n",
            "    Uninstalling filelock-3.9.0:\n",
            "      Successfully uninstalled filelock-3.9.0\n",
            "  Attempting uninstall: einops\n",
            "    Found existing installation: einops 0.6.0\n",
            "    Uninstalling einops-0.6.0:\n",
            "      Successfully uninstalled einops-0.6.0\n",
            "  Attempting uninstall: cython\n",
            "    Found existing installation: Cython 0.29.33\n",
            "    Uninstalling Cython-0.29.33:\n",
            "      Successfully uninstalled Cython-0.29.33\n",
            "  Attempting uninstall: colorama\n",
            "    Found existing installation: colorama 0.4.6\n",
            "    Uninstalling colorama-0.4.6:\n",
            "      Successfully uninstalled colorama-0.4.6\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2022.12.7\n",
            "    Uninstalling certifi-2022.12.7:\n",
            "      Successfully uninstalled certifi-2022.12.7\n",
            "  Attempting uninstall: beartype\n",
            "    Found existing installation: beartype 0.12.0\n",
            "    Uninstalling beartype-0.12.0:\n",
            "      Successfully uninstalled beartype-0.12.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.10.1\n",
            "    Uninstalling scipy-1.10.1:\n",
            "      Successfully uninstalled scipy-1.10.1\n",
            "  Attempting uninstall: sacrebleu\n",
            "    Found existing installation: sacrebleu 2.3.1\n",
            "    Uninstalling sacrebleu-2.3.1:\n",
            "      Successfully uninstalled sacrebleu-2.3.1\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.28.2\n",
            "    Uninstalling requests-2.28.2:\n",
            "      Successfully uninstalled requests-2.28.2\n",
            "  Attempting uninstall: omegaconf\n",
            "    Found existing installation: omegaconf 2.0.6\n",
            "    Uninstalling omegaconf-2.0.6:\n",
            "      Successfully uninstalled omegaconf-2.0.6\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu11\n",
            "    Found existing installation: nvidia-cuda-runtime-cu11 11.7.99\n",
            "    Uninstalling nvidia-cuda-runtime-cu11-11.7.99:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu11-11.7.99\n",
            "  Attempting uninstall: nvidia-cublas-cu11\n",
            "    Found existing installation: nvidia-cublas-cu11 11.10.3.66\n",
            "    Uninstalling nvidia-cublas-cu11-11.10.3.66:\n",
            "      Successfully uninstalled nvidia-cublas-cu11-11.10.3.66\n",
            "  Attempting uninstall: cffi\n",
            "    Found existing installation: cffi 1.15.1\n",
            "    Uninstalling cffi-1.15.1:\n",
            "      Successfully uninstalled cffi-1.15.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.1\n",
            "    Uninstalling scikit-learn-1.2.1:\n",
            "      Successfully uninstalled scikit-learn-1.2.1\n",
            "  Attempting uninstall: nvidia-cudnn-cu11\n",
            "    Found existing installation: nvidia-cudnn-cu11 8.5.0.96\n",
            "    Uninstalling nvidia-cudnn-cu11-8.5.0.96:\n",
            "      Successfully uninstalled nvidia-cudnn-cu11-8.5.0.96\n",
            "  Attempting uninstall: hydra-core\n",
            "    Found existing installation: hydra-core 1.0.7\n",
            "    Uninstalling hydra-core-1.0.7:\n",
            "      Successfully uninstalled hydra-core-1.0.7\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.12.1\n",
            "    Uninstalling huggingface-hub-0.12.1:\n",
            "      Successfully uninstalled huggingface-hub-0.12.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.26.1\n",
            "    Uninstalling transformers-4.26.1:\n",
            "      Successfully uninstalled transformers-4.26.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1\n",
            "    Uninstalling torch-1.13.1:\n",
            "      Successfully uninstalled torch-1.13.1\n",
            "  Attempting uninstall: vector-quantize-pytorch\n",
            "    Found existing installation: vector-quantize-pytorch 1.0.1\n",
            "    Uninstalling vector-quantize-pytorch-1.0.1:\n",
            "      Successfully uninstalled vector-quantize-pytorch-1.0.1\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.13.1\n",
            "    Uninstalling torchaudio-0.13.1:\n",
            "      Successfully uninstalled torchaudio-0.13.1\n",
            "  Attempting uninstall: Mega-pytorch\n",
            "    Found existing installation: Mega-pytorch 0.0.15\n",
            "    Uninstalling Mega-pytorch-0.0.15:\n",
            "      Successfully uninstalled Mega-pytorch-0.0.15\n",
            "  Attempting uninstall: local-attention\n",
            "    Found existing installation: local-attention 1.6.0\n",
            "    Uninstalling local-attention-1.6.0:\n",
            "      Successfully uninstalled local-attention-1.6.0\n",
            "  Attempting uninstall: lion-pytorch\n",
            "    Found existing installation: lion-pytorch 0.0.6\n",
            "    Uninstalling lion-pytorch-0.0.6:\n",
            "      Successfully uninstalled lion-pytorch-0.0.6\n",
            "  Attempting uninstall: ema-pytorch\n",
            "    Found existing installation: ema-pytorch 0.2.1\n",
            "    Uninstalling ema-pytorch-0.2.1:\n",
            "      Successfully uninstalled ema-pytorch-0.2.1\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 0.16.0\n",
            "    Uninstalling accelerate-0.16.0:\n",
            "      Successfully uninstalled accelerate-0.16.0\n",
            "  Attempting uninstall: fairseq\n",
            "    Found existing installation: fairseq 0.12.2\n",
            "    Uninstalling fairseq-0.12.2:\n",
            "      Successfully uninstalled fairseq-0.12.2\n",
            "  Attempting uninstall: audiolm-pytorch\n",
            "    Found existing installation: audiolm-pytorch 0.14.3\n",
            "    Uninstalling audiolm-pytorch-0.14.3:\n",
            "      Successfully uninstalled audiolm-pytorch-0.14.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.2 which is incompatible.\n",
            "aiohttp 3.8.3 requires charset-normalizer<3.0,>=2.0, but you have charset-normalizer 3.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Mega-pytorch-0.0.15 accelerate-0.16.0 antlr4-python3-runtime-4.8 audiolm-pytorch-0.14.3 beartype-0.12.0 bitarray-2.7.2 certifi-2022.12.7 cffi-1.15.1 charset-normalizer-3.0.1 colorama-0.4.6 cython-0.29.33 einops-0.6.0 ema-pytorch-0.2.1 fairseq-0.12.2 filelock-3.9.0 huggingface-hub-0.12.1 hydra-core-1.0.7 idna-3.4 joblib-1.2.0 lion-pytorch-0.0.6 local-attention-1.6.0 lxml-4.9.2 numpy-1.24.2 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 omegaconf-2.0.6 packaging-23.0 portalocker-2.7.0 psutil-5.9.4 pycparser-2.21 pyyaml-6.0 regex-2022.10.31 requests-2.28.2 sacrebleu-2.3.1 scikit-learn-1.2.1 scipy-1.10.1 sentencepiece-0.1.97 setuptools-67.3.2 tabulate-0.9.0 threadpoolctl-3.1.0 tokenizers-0.13.2 torch-1.13.1 torchaudio-0.13.1 tqdm-4.64.1 transformers-4.26.1 typing-extensions-4.5.0 urllib3-1.26.14 vector-quantize-pytorch-1.0.1 wheel-0.38.4\n"
          ]
        }
      ],
      "source": [
        "!pip install audiolm-pytorch\n",
        "#!pip install --upgrade --force-reinstall audiolm-pytorch"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xuNcsDJsvQwh"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBxNK5cKW--_"
      },
      "source": [
        "### Imports & paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OrNeKngVVM0L"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import math\n",
        "import wave\n",
        "import struct\n",
        "import os\n",
        "import urllib.request\n",
        "import tarfile\n",
        "from audiolm_pytorch import SoundStream, SoundStreamTrainer, HubertWithKmeans, SemanticTransformer, SemanticTransformerTrainer, HubertWithKmeans, CoarseTransformer, CoarseTransformerWrapper, CoarseTransformerTrainer, FineTransformer, FineTransformerWrapper, FineTransformerTrainer, AudioLM\n",
        "from torch import nn\n",
        "import torch\n",
        "import torchaudio\n",
        "\n",
        "\n",
        "# define all dataset paths, checkpoints, etc\n",
        "dataset_folder = \"../../datasets/fma_medium/\"\n",
        "soundstream_ckpt = \"runs/soundstream.20200_320.pt\" # this can change depending on number of steps\n",
        "hubert_ckpt = 'hubert/hubert_base_ls960.pt'\n",
        "hubert_quantizer = f'hubert/hubert_base_ls960_L9_km500.bin' # listed in row \"HuBERT Base (~95M params)\", column Quantizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYcI0aXEwuxR"
      },
      "source": [
        "## Training\n",
        "\n",
        "Now that we have a dataset, we can train AudioLM.\n",
        "\n",
        "**Note**: do NOT type \"y\" to overwrite previous experiments/ checkpoints when running through the cells here unless you're ready to the entire results folder! Otherwise you will end up erasing things (e.g. you train SoundStream first, and if you choose \"overwrite\" then you lose the SoundStream checkpoint when you then train SemanticTransformer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7GiyBcBWiZV"
      },
      "source": [
        "### SoundStream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGU0OZiOwPEO",
        "outputId": "21dd959c-6458-4477-8403-cf810166f38d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training with dataset of 23729 samples and validating with randomly splitted 1249 samples\n",
            "0: saving to results\n",
            "0: saving model to results\n",
            "100: saving to results\n",
            "100: saving model to results\n",
            "200: saving to results\n",
            "200: saving model to results\n",
            "300: saving to results\n",
            "300: saving model to results\n",
            "400: saving to results\n",
            "400: saving model to results\n",
            "500: saving to results\n",
            "500: saving model to results\n",
            "600: saving to results\n",
            "600: saving model to results\n",
            "700: saving to results\n",
            "700: saving model to results\n",
            "800: saving to results\n",
            "800: saving model to results\n",
            "900: saving to results\n",
            "900: saving model to results\n",
            "1000: saving to results\n",
            "1000: saving model to results\n",
            "1100: saving to results\n",
            "1100: saving model to results\n",
            "1200: saving to results\n",
            "1200: saving model to results\n",
            "1300: saving to results\n",
            "1300: saving model to results\n",
            "1400: saving to results\n",
            "1400: saving model to results\n",
            "1500: saving to results\n",
            "1500: saving model to results\n",
            "1600: saving to results\n",
            "1600: saving model to results\n",
            "1700: saving to results\n",
            "1700: saving model to results\n",
            "1800: saving to results\n",
            "1800: saving model to results\n",
            "1900: saving to results\n",
            "1900: saving model to results\n",
            "2000: saving to results\n",
            "2000: saving model to results\n",
            "2100: saving to results\n",
            "2100: saving model to results\n",
            "2200: saving to results\n",
            "2200: saving model to results\n",
            "2300: saving to results\n",
            "2300: saving model to results\n",
            "2400: saving to results\n",
            "2400: saving model to results\n",
            "2500: saving to results\n",
            "2500: saving model to results\n",
            "2600: saving to results\n",
            "2600: saving model to results\n",
            "2700: saving to results\n",
            "2700: saving model to results\n",
            "2800: saving to results\n",
            "2800: saving model to results\n",
            "2900: saving to results\n",
            "2900: saving model to results\n",
            "3000: saving to results\n",
            "3000: saving model to results\n",
            "3100: saving to results\n",
            "3100: saving model to results\n",
            "3200: saving to results\n",
            "3200: saving model to results\n",
            "3300: saving to results\n",
            "3300: saving model to results\n",
            "3400: saving to results\n",
            "3400: saving model to results\n",
            "3500: saving to results\n",
            "3500: saving model to results\n",
            "3600: saving to results\n",
            "3600: saving model to results\n",
            "3700: saving to results\n",
            "3700: saving model to results\n",
            "3800: saving to results\n",
            "3800: saving model to results\n",
            "3900: saving to results\n",
            "3900: saving model to results\n",
            "4000: saving to results\n",
            "4000: saving model to results\n",
            "4100: saving to results\n",
            "4100: saving model to results\n",
            "4200: saving to results\n",
            "4200: saving model to results\n",
            "4300: saving to results\n",
            "4300: saving model to results\n",
            "4400: saving to results\n",
            "4400: saving model to results\n",
            "4500: saving to results\n",
            "4500: saving model to results\n",
            "4600: saving to results\n",
            "4600: saving model to results\n",
            "4700: saving to results\n",
            "4700: saving model to results\n",
            "4800: saving to results\n",
            "4800: saving model to results\n",
            "4900: saving to results\n",
            "4900: saving model to results\n",
            "5000: saving to results\n",
            "5000: saving model to results\n",
            "5100: saving to results\n",
            "5100: saving model to results\n",
            "5200: saving to results\n",
            "5200: saving model to results\n",
            "5300: saving to results\n",
            "5300: saving model to results\n",
            "5400: saving to results\n",
            "5400: saving model to results\n",
            "5500: saving to results\n",
            "5500: saving model to results\n",
            "5600: saving to results\n",
            "5600: saving model to results\n",
            "5700: saving to results\n",
            "5700: saving model to results\n",
            "5800: saving to results\n",
            "5800: saving model to results\n",
            "5900: saving to results\n",
            "5900: saving model to results\n",
            "6000: saving to results\n",
            "6000: saving model to results\n",
            "6100: saving to results\n",
            "6100: saving model to results\n",
            "6200: saving to results\n",
            "6200: saving model to results\n",
            "6300: saving to results\n",
            "6300: saving model to results\n",
            "6400: saving to results\n",
            "6400: saving model to results\n",
            "6500: saving to results\n",
            "6500: saving model to results\n",
            "6600: saving to results\n",
            "6600: saving model to results\n",
            "6700: saving to results\n",
            "6700: saving model to results\n",
            "6800: saving to results\n",
            "6800: saving model to results\n",
            "6900: saving to results\n",
            "6900: saving model to results\n",
            "7000: saving to results\n",
            "7000: saving model to results\n",
            "7100: saving to results\n",
            "7100: saving model to results\n",
            "7200: saving to results\n",
            "7200: saving model to results\n",
            "7300: saving to results\n",
            "7300: saving model to results\n",
            "7400: saving to results\n",
            "7400: saving model to results\n",
            "7500: saving to results\n",
            "7500: saving model to results\n",
            "7600: saving to results\n",
            "7600: saving model to results\n",
            "7700: saving to results\n",
            "7700: saving model to results\n",
            "7800: saving to results\n",
            "7800: saving model to results\n",
            "7900: saving to results\n",
            "7900: saving model to results\n",
            "8000: saving to results\n",
            "8000: saving model to results\n",
            "8100: saving to results\n",
            "8100: saving model to results\n",
            "8200: saving to results\n",
            "8200: saving model to results\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_730190/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">341436423.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">22</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_730190/341436423.py'</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/lagodish/Dev/audiolm-pytorch/audiolm_pytorch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">454</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 451 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, log_fn = noop):                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 452 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 453 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">while</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.steps &lt; <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.num_train_steps:                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 454 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>logs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.train_step()                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 455 │   │   │   </span>log_fn(logs)                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 456 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 457 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.print(<span style=\"color: #808000; text-decoration-color: #808000\">'training complete'</span>)                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/lagodish/Dev/audiolm-pytorch/audiolm_pytorch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">368</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train_step</span>                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 365 │   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 366 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> name, discr_loss <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> discr_losses:                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 367 │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.accelerator.backward(discr_loss / <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.grad_accum_every, retain_gra  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 368 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>accum_log(logs, {name: discr_loss.item() / <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.grad_accum_every})        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 369 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 370 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> exists(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.discr_max_grad_norm):                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 371 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.accelerator.clip_grad_norm_(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.soundstream.stft_discriminator.paramet  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_730190/\u001b[0m\u001b[1;33m341436423.py\u001b[0m:\u001b[94m22\u001b[0m in \u001b[92m<module>\u001b[0m                                                \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_730190/341436423.py'\u001b[0m                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/home/lagodish/Dev/audiolm-pytorch/audiolm_pytorch/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m454\u001b[0m in \u001b[92mtrain\u001b[0m                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 451 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mtrain\u001b[0m(\u001b[96mself\u001b[0m, log_fn = noop):                                                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 452 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 453 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwhile\u001b[0m \u001b[96mself\u001b[0m.steps < \u001b[96mself\u001b[0m.num_train_steps:                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 454 \u001b[2m│   │   │   \u001b[0mlogs = \u001b[96mself\u001b[0m.train_step()                                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 455 \u001b[0m\u001b[2m│   │   │   \u001b[0mlog_fn(logs)                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 456 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 457 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.print(\u001b[33m'\u001b[0m\u001b[33mtraining complete\u001b[0m\u001b[33m'\u001b[0m)                                                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/home/lagodish/Dev/audiolm-pytorch/audiolm_pytorch/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m368\u001b[0m in \u001b[92mtrain_step\u001b[0m                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 365 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 366 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mfor\u001b[0m name, discr_loss \u001b[95min\u001b[0m discr_losses:                                         \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 367 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.accelerator.backward(discr_loss / \u001b[96mself\u001b[0m.grad_accum_every, retain_gra  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 368 \u001b[2m│   │   │   │   \u001b[0maccum_log(logs, {name: discr_loss.item() / \u001b[96mself\u001b[0m.grad_accum_every})        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 369 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 370 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m exists(\u001b[96mself\u001b[0m.discr_max_grad_norm):                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 371 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.accelerator.clip_grad_norm_(\u001b[96mself\u001b[0m.soundstream.stft_discriminator.paramet  \u001b[31m│\u001b[0m\n",
              "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
              "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "soundstream = SoundStream(\n",
        "    codebook_size = 1024,\n",
        "    rq_num_quantizers = 8,\n",
        "    target_sample_hz = 24000,\n",
        "    strides = (2, 4, 5, 8)\n",
        ")\n",
        "\n",
        "soundstream.load_from_trainer_saved_obj(f\"./{soundstream_ckpt}\")  #Load pretrained\n",
        "\n",
        "trainer = SoundStreamTrainer(\n",
        "    soundstream,\n",
        "    folder = dataset_folder,\n",
        "    batch_size = 4,\n",
        "    grad_accum_every = 8,         # effective batch size of 32\n",
        "    data_max_length = 320 * 32,\n",
        "    save_results_every = 100,\n",
        "    save_model_every = 100,\n",
        "    num_train_steps = 25005,\n",
        "    random_split_seed=1112\n",
        ").cuda()\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SoundStream Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "from audiolm_pytorch.data import SoundDataset, get_dataloader\n",
        "from audiolm_pytorch.trainer import cycle\n",
        "from pathlib import Path\n",
        "\n",
        "ds = SoundDataset(\n",
        "            folder = dataset_folder,\n",
        "            max_length = 24000*15,  #15 Sec\n",
        "            target_sample_hz = soundstream.target_sample_hz,\n",
        "            seq_len_multiple_of = soundstream.seq_len_multiple_of\n",
        ")\n",
        "\n",
        "dl = get_dataloader(ds, batch_size = 1, num_workers = 0, shuffle = True)\n",
        "\n",
        "dl_iter = cycle(dl)\n",
        "\n",
        "wave, = next(dl_iter)\n",
        "wave = wave.to(\"cuda\")\n",
        "\n",
        "\n",
        "results_folder = Path('./results_test')\n",
        "results_folder.mkdir(exist_ok=True)\n",
        "filename = str(results_folder / f'orig.wav')\n",
        "torchaudio.save(filename, wave.cpu(), soundstream.target_sample_hz)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "soundstream = SoundStream(\n",
        "    codebook_size = 1024,\n",
        "    rq_num_quantizers = 12,\n",
        "    target_sample_hz = 24000,\n",
        "    strides = (3, 4, 5, 8)\n",
        ")\n",
        "\n",
        "for i in range(9300,9400,100):\n",
        "    soundstream_path = f\"results/soundstream.{i}.pt\"\n",
        "    soundstream.load_from_trainer_saved_obj(f\"./{soundstream_path}\")  #Load ckpt for test\n",
        "\n",
        "    soundstream.eval()\n",
        "    soundstream = soundstream.to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        recons = soundstream(wave, return_recons_only = True)\n",
        "\n",
        "    for ind, recon in enumerate(recons.unbind(dim = 0)):\n",
        "        filename = str(results_folder / f'sample_{i}.wav')\n",
        "        torchaudio.save(filename, recon.cpu().detach(), soundstream.target_sample_hz)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqjN28L4Wc5Q"
      },
      "source": [
        "### SemanticTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgd962eSvDzS",
        "outputId": "b0550cde-0c8b-4a39-f896-f6f813f50f8c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MiniBatchKMeans from version 0.24.0 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training with dataset of 2 samples and validating with randomly splitted 1 samples\n",
            "do you want to clear previous experiment checkpoints and results? (y/n) n\n",
            "0: loss: 6.648584365844727\n",
            "0: valid loss 5.763116359710693\n",
            "0: saving model to results\n",
            "training complete\n"
          ]
        }
      ],
      "source": [
        "# hubert checkpoints can be downloaded at\n",
        "# https://github.com/facebookresearch/fairseq/tree/main/examples/hubert\n",
        "if not os.path.isdir(\"hubert\"):\n",
        "  os.makedirs(\"hubert\")\n",
        "if not os.path.isfile(hubert_ckpt):\n",
        "  hubert_ckpt_download = f\"https://dl.fbaipublicfiles.com/{hubert_ckpt}\"\n",
        "  urllib.request.urlretrieve(hubert_ckpt_download, f\"./{hubert_ckpt}\")\n",
        "if not os.path.isfile(hubert_quantizer):\n",
        "  hubert_quantizer_download = f\"https://dl.fbaipublicfiles.com/{hubert_quantizer}\"\n",
        "  urllib.request.urlretrieve(hubert_quantizer_download, f\"./{hubert_quantizer}\")\n",
        "\n",
        "wav2vec = HubertWithKmeans(\n",
        "    checkpoint_path = f'./{hubert_ckpt}',\n",
        "    kmeans_path = f'./{hubert_quantizer}'\n",
        ")\n",
        "\n",
        "semantic_transformer = SemanticTransformer(\n",
        "    num_semantic_tokens = wav2vec.codebook_size,\n",
        "    dim = 1024,\n",
        "    depth = 6\n",
        ").cuda()\n",
        "\n",
        "\n",
        "trainer = SemanticTransformerTrainer(\n",
        "    transformer = semantic_transformer,\n",
        "    wav2vec = wav2vec,\n",
        "    folder = dataset_folder,\n",
        "    batch_size = 1,\n",
        "    data_max_length = 320 * 32,\n",
        "    num_train_steps = 1\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eEvIzhEWwRz"
      },
      "source": [
        "### CoarseTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LeWmaNHzzY9",
        "outputId": "7e7ecb3b-f59e-4d18-c8c9-64762e9b43fc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator MiniBatchKMeans from version 0.24.0 when using version 1.0.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/modules/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training with dataset of 2 samples and validating with randomly splitted 1 samples\n",
            "do you want to clear previous experiment checkpoints and results? (y/n) n\n",
            "0: loss: 63.983970642089844\n",
            "0: valid loss 63.398582458496094\n",
            "0: saving model to results\n",
            "1: loss: 65.85967254638672\n",
            "2: loss: 62.4722900390625\n",
            "2: valid loss 50.01605987548828\n",
            "3: loss: 11.735434532165527\n",
            "4: loss: 3.976104497909546\n",
            "4: valid loss 46.094608306884766\n",
            "4: saving model to results\n",
            "5: loss: 58.27140426635742\n",
            "6: loss: 41.68347930908203\n",
            "6: valid loss 45.54595184326172\n",
            "7: loss: 2.2387890815734863\n",
            "8: loss: 0.4718627631664276\n",
            "8: valid loss 39.10848617553711\n",
            "8: saving model to results\n",
            "training complete\n"
          ]
        }
      ],
      "source": [
        "wav2vec = HubertWithKmeans(\n",
        "    checkpoint_path = f'./{hubert_ckpt}',\n",
        "    kmeans_path = f'./{hubert_quantizer}'\n",
        ")\n",
        "\n",
        "soundstream = SoundStream(\n",
        "    codebook_size = 1024,\n",
        "    rq_num_quantizers = 8,\n",
        ")\n",
        "\n",
        "soundstream.load(f\"./{soundstream_ckpt}\")\n",
        "\n",
        "coarse_transformer = CoarseTransformer(\n",
        "    num_semantic_tokens = wav2vec.codebook_size,\n",
        "    codebook_size = 1024,\n",
        "    num_coarse_quantizers = 3,\n",
        "    dim = 512,\n",
        "    depth = 6\n",
        ")\n",
        "\n",
        "trainer = CoarseTransformerTrainer(\n",
        "    transformer = coarse_transformer,\n",
        "    soundstream = soundstream,\n",
        "    wav2vec = wav2vec,\n",
        "    folder = dataset_folder,\n",
        "    batch_size = 1,\n",
        "    data_max_length = 320 * 32,\n",
        "    save_results_every = 2,\n",
        "    save_model_every = 4,\n",
        "    num_train_steps = 9\n",
        ")\n",
        "# NOTE: I changed num_train_steps to 9 (aka 8 + 1) from 10000 to make things go faster for demo purposes\n",
        "# adjusting save_*_every variables for the same reason\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRvj7qOJWzmw"
      },
      "source": [
        "### FineTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRaEhRRKWg8F",
        "outputId": "7cc166c4-c8e9-45ef-8293-8f5381c2d3af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training with dataset of 2 samples and validating with randomly splitted 1 samples\n",
            "do you want to clear previous experiment checkpoints and results? (y/n) n\n",
            "0: loss: 70.90608215332031\n",
            "0: valid loss 65.99951171875\n",
            "0: saving model to results\n",
            "1: loss: 43.6014289855957\n",
            "2: loss: 8.300681114196777\n",
            "3: loss: 61.23375701904297\n",
            "4: loss: 63.34052276611328\n",
            "5: loss: 2.010118246078491\n",
            "6: loss: 56.52588653564453\n",
            "7: loss: 0.5423888564109802\n",
            "8: loss: 0.005095238331705332\n",
            "training complete\n"
          ]
        }
      ],
      "source": [
        "soundstream = SoundStream(\n",
        "    codebook_size = 1024,\n",
        "    rq_num_quantizers = 8,\n",
        ")\n",
        "\n",
        "soundstream.load(f\"./{soundstream_ckpt}\")\n",
        "\n",
        "fine_transformer = FineTransformer(\n",
        "    num_coarse_quantizers = 3,\n",
        "    num_fine_quantizers = 5,\n",
        "    codebook_size = 1024,\n",
        "    dim = 512,\n",
        "    depth = 6\n",
        ")\n",
        "\n",
        "trainer = FineTransformerTrainer(\n",
        "    transformer = fine_transformer,\n",
        "    soundstream = soundstream,\n",
        "    folder = dataset_folder,\n",
        "    batch_size = 1,\n",
        "    data_max_length = 320 * 32,\n",
        "    num_train_steps = 9\n",
        ")\n",
        "# NOTE: I changed num_train_steps to 9 (aka 8 + 1) from 10000 to make things go faster for demo purposes\n",
        "# adjusting save_*_every variables for the same reason\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoHgkgA3XKXH"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzghrux5WinW",
        "outputId": "9dd39f7f-0046-4a5f-826e-a442345987af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "generating semantic:   0%|          | 10/2048 [00:00<00:25, 78.55it/s]\n",
            "generating coarse: 100%|██████████| 512/512 [00:14<00:00, 34.83it/s]\n",
            "generating fine: 100%|██████████| 512/512 [02:56<00:00,  2.91it/s]\n"
          ]
        }
      ],
      "source": [
        "# Everything together\n",
        "audiolm = AudioLM(\n",
        "    wav2vec = wav2vec,\n",
        "    soundstream = soundstream,\n",
        "    semantic_transformer = semantic_transformer,\n",
        "    coarse_transformer = coarse_transformer,\n",
        "    fine_transformer = fine_transformer\n",
        ")\n",
        "\n",
        "generated_wav = audiolm(batch_size = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "4rQPHTSRngEr"
      },
      "outputs": [],
      "source": [
        "output_path = \"out.wav\"\n",
        "sample_rate = 44100\n",
        "torchaudio.save(output_path, generated_wav.cpu(), sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "is9wLY_ncDYK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "MusicEnv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "bffe47f1c9f0e7fbf3f698259a3ec592739737d187a49ddf7120ee60a480d3a6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
